{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cb2b9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import HDBSCAN\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "data = pd.read_csv(r'C:\\Users\\jdorv\\Coding Fun\\Bank Innovation\\wrds_bank_data_MERGED.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "21cb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_split_columns(df):\n",
    "    \"\"\"\n",
    "    Merge RCFD/RCON columns that split in 2011.\n",
    "    RCFD (consolidated) has data pre-2011, RCON (domestic) has data post-2011.\n",
    "    Strategy: Create merged columns using RCFD when available, fill with RCON otherwise.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"MERGING SPLIT TIME CODES (2011 Transition)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Define split pairs: (rcfd_col, rcon_col, new_merged_name)\n",
    "    split_pairs = [\n",
    "        ('rcfd2_rcfd2170', 'rcon2_rcon2170', 'total_assets'),\n",
    "        ('rcfd2_rcfd2122', 'rcon2_rcon2122', 'total_loans'),\n",
    "        ('rcfd2_rcfd3210', 'rcon2_rcon3210', 'total_equity'),\n",
    "        ('rcfd1_rcfd3123', 'rcon1_rcon3123', 'allowance_loan_losses'),\n",
    "        ('rcfd1_rcfd1590', 'rcon1_rcon1590', 'agricultural_loans'),\n",
    "        ('rcfd1_rcfd1754', 'rcon1_rcon1754', 'htm_securities'),\n",
    "        ('rcfd1_rcfd1773', 'rcon1_rcon1773', 'afs_securities'),\n",
    "        ('rcfd1_rcfd2150', 'rcon2_rcon2150', 'goodwill'),\n",
    "        ('rcfd1_rcfd0081', 'rcon2_rcon0081', 'cash_items_process'),\n",
    "        ('rcfd2_rcfd1420', 'rcon2_rcon1420', 'farmland_loans'),\n",
    "        ('rcfd2_rcfd1460', 'rcon2_rcon1460', 'multifamily_loans'),\n",
    "    ]\n",
    "    \n",
    "    merged_count = 0\n",
    "    for rcfd_col, rcon_col, new_name in split_pairs:\n",
    "        # Check if both columns exist\n",
    "        rcfd_exists = rcfd_col in df.columns\n",
    "        rcon_exists = rcon_col in df.columns\n",
    "        \n",
    "        if rcfd_exists and rcon_exists:\n",
    "            # Merge: use RCFD, fill missing with RCON\n",
    "            df[new_name] = df[rcfd_col].fillna(df[rcon_col])\n",
    "            \n",
    "            # Count how many values came from each source\n",
    "            rcfd_count = df[rcfd_col].notna().sum()\n",
    "            rcon_count = df[rcon_col].notna().sum()\n",
    "            merged_total = df[new_name].notna().sum()\n",
    "            \n",
    "            print(f\"‚úì {new_name:25s} | RCFD: {rcfd_count:>6,} | RCON: {rcon_count:>6,} | Total: {merged_total:>6,}\")\n",
    "            \n",
    "            # Drop original columns to avoid confusion\n",
    "            df = df.drop(columns=[rcfd_col, rcon_col])\n",
    "            merged_count += 1\n",
    "            \n",
    "        elif rcfd_exists and not rcon_exists:\n",
    "            # Only RCFD exists, just rename it\n",
    "            df[new_name] = df[rcfd_col]\n",
    "            df = df.drop(columns=[rcfd_col])\n",
    "            print(f\"‚ö†Ô∏è  {new_name:25s} | Only RCFD exists, renamed\")\n",
    "            merged_count += 1\n",
    "            \n",
    "        elif rcon_exists and not rcfd_exists:\n",
    "            # Only RCON exists, just rename it\n",
    "            df[new_name] = df[rcon_col]\n",
    "            df = df.drop(columns=[rcon_col])\n",
    "            print(f\"‚ö†Ô∏è  {new_name:25s} | Only RCON exists, renamed\")\n",
    "            merged_count += 1\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {new_name:25s} | Neither column found, skipping\")\n",
    "    \n",
    "    print(f\"\\n‚úì Successfully merged/renamed {merged_count} split column pairs\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_ratios(df):\n",
    "    \"\"\"Calculate ratios using merged columns\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CALCULATING FINANCIAL RATIOS (USING MERGED COLUMNS)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    ratios = df.copy()\n",
    "    \n",
    "    def safe_divide(numerator, denominator):\n",
    "        result = numerator / denominator\n",
    "        result = result.replace([np.inf, -np.inf], np.nan)\n",
    "        return result\n",
    "    \n",
    "    # === CORE INNOVATION METRICS (3 ratios) ===\n",
    "    ratios['tech_investment_ratio'] = safe_divide(\n",
    "        df['riad4092'], df['total_assets']\n",
    "    ) * 1000\n",
    "    print(\"‚úì Tech Investment Ratio\")\n",
    "    \n",
    "    ratios['nib_deposit_ratio'] = safe_divide(\n",
    "        df['rcon2_rcon6631'], df['rcon2_rcon2200']\n",
    "    ) * 100\n",
    "    print(\"‚úì NIB Deposit Ratio (Digital Banking Proxy)\")\n",
    "    \n",
    "    ratios['service_charge_intensity'] = safe_divide(\n",
    "        df['riad4080'], df['rcon2_rcon2200']\n",
    "    ) * 1000\n",
    "    print(\"‚úì Service Charge Intensity\")\n",
    "    \n",
    "    # === EFFICIENCY METRICS (2 ratios) ===\n",
    "    ratios['efficiency_ratio'] = safe_divide(\n",
    "        df['riad4093'], df['riad4074'] + df['riad4079']\n",
    "    ) * 100\n",
    "    print(\"‚úì Efficiency Ratio\")\n",
    "    \n",
    "    ratios['nonint_income_pct'] = safe_divide(\n",
    "        df['riad4079'], df['riad4074'] + df['riad4079']\n",
    "    ) * 100\n",
    "    print(\"‚úì Noninterest Income %\")\n",
    "    \n",
    "    # === BALANCE SHEET (3 ratios) ===\n",
    "    ratios['loans_to_assets'] = safe_divide(\n",
    "        df['total_loans'], df['total_assets']\n",
    "    ) * 100\n",
    "    print(\"‚úì Loans-to-Assets\")\n",
    "    \n",
    "    ratios['equity_to_assets'] = safe_divide(\n",
    "        df['total_equity'], df['total_assets']\n",
    "    ) * 100\n",
    "    print(\"‚úì Equity-to-Assets\")\n",
    "    \n",
    "    ratios['deposits_to_assets'] = safe_divide(\n",
    "        df['rcon2_rcon2200'], df['total_assets']\n",
    "    ) * 100\n",
    "    print(\"‚úì Deposits-to-Assets\")\n",
    "    \n",
    "    # === PROFITABILITY (2 ratios) ===\n",
    "    ratios['roa'] = safe_divide(\n",
    "        df['riad4340'], df['total_assets']\n",
    "    ) * 100\n",
    "    print(\"‚úì ROA\")\n",
    "    \n",
    "    ratios['roe'] = safe_divide(\n",
    "        df['riad4340'], df['total_equity']\n",
    "    ) * 100\n",
    "    print(\"‚úì ROE\")\n",
    "    \n",
    "    # === DEPOSIT MIX (1 ratio) ===\n",
    "    ratios['nontrans_deposits_pct'] = safe_divide(\n",
    "        df['rcon2_rcon2215'], df['rcon2_rcon2200']\n",
    "    ) * 100\n",
    "    print(\"‚úì Nontransaction Deposits %\")\n",
    "    \n",
    "    print(f\"\\n‚úì Created 11 ratios using merged columns\")\n",
    "    return ratios\n",
    "\n",
    "\n",
    "def prepare_clustering_features(df):\n",
    "    \"\"\"Select universal coverage features AND keep identifiers\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PREPARING CLUSTERING FEATURES - UNIVERSAL COVERAGE ONLY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # RIAD columns - Universal coverage (<2% missing)\n",
    "    riad_universal = [\n",
    "        'riad4010', 'riad4012', 'riad4020', 'riad4073', 'riad4074', 'riad4079',\n",
    "        'riad4080', 'riad4092', 'riad4093', 'riad4107', 'riad4115', 'riad4135',\n",
    "        'riad4150', 'riad4180', 'riad4217', 'riad4230', 'riad4266', 'riad4267',\n",
    "        'riad4300', 'riad4301', 'riad4302', 'riad4313', 'riad4340', 'riad4356',\n",
    "        'riad4415', 'riad4435', 'riad4436', 'riad4460', 'riad4470', 'riad4498',\n",
    "        'riad4499', 'riad4507', 'riad4508', 'riad4518', 'riad4605', 'riad4608',\n",
    "        'riad4628', 'riad4635', 'riad4638', 'riad4644', 'riad4769'\n",
    "    ]\n",
    "    \n",
    "    # RCON columns - Universal coverage (<2% missing)\n",
    "    # NOTE: Split columns have been merged, so we now use merged names\n",
    "    rcon_universal = [\n",
    "        'rcon2_rcon2200',  # Total deposits (no split)\n",
    "        'rcon2_rcon2202',  # Transaction accounts (no split)\n",
    "        'rcon2_rcon2215',  # Nontransaction accounts (no split)\n",
    "        'rcon2_rcon6631',  # NIB deposits (no split)\n",
    "        'rcon1_rcon1766',  # C&I loans (no split)\n",
    "    ]\n",
    "    \n",
    "    # Merged columns (from split pairs)\n",
    "    merged_columns = [\n",
    "        'total_assets',\n",
    "        'total_loans',\n",
    "        'total_equity',\n",
    "        'allowance_loan_losses',\n",
    "        'agricultural_loans',\n",
    "        'htm_securities',\n",
    "        'afs_securities',\n",
    "        'goodwill',\n",
    "        'cash_items_process',\n",
    "        'farmland_loans',\n",
    "        'multifamily_loans',\n",
    "    ]\n",
    "    \n",
    "    # Ratios - Successfully calculated (<2% missing)\n",
    "    ratios_clean = [\n",
    "        'tech_investment_ratio',\n",
    "        'nib_deposit_ratio',\n",
    "        'service_charge_intensity',\n",
    "        'efficiency_ratio',\n",
    "        'nonint_income_pct',\n",
    "        'loans_to_assets',\n",
    "        'equity_to_assets',\n",
    "        'deposits_to_assets',\n",
    "        'roa',\n",
    "        'roe',\n",
    "        'nontrans_deposits_pct'\n",
    "    ]\n",
    "    \n",
    "    # Identifiers - MUST KEEP THESE\n",
    "    identifiers = ['rssd9001', 'rssd9999', 'rssd9017', 'year', 'quarter']\n",
    "    \n",
    "    # Check which identifiers exist\n",
    "    existing_identifiers = [col for col in identifiers if col in df.columns]\n",
    "    if len(existing_identifiers) < len(identifiers):\n",
    "        missing = [col for col in identifiers if col not in df.columns]\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Missing identifier columns: {missing}\")\n",
    "    \n",
    "    # Combine all feature columns\n",
    "    feature_cols = riad_universal + rcon_universal + merged_columns + ratios_clean\n",
    "    \n",
    "    # Check which features actually exist\n",
    "    existing_features = [col for col in feature_cols if col in df.columns]\n",
    "    missing_features = [col for col in feature_cols if col not in df.columns]\n",
    "    \n",
    "    print(f\"\\nüìä Feature selection:\")\n",
    "    print(f\"  Expected: {len(feature_cols)} features\")\n",
    "    print(f\"  Found: {len(existing_features)} features\")\n",
    "    if missing_features:\n",
    "        print(f\"  Missing: {len(missing_features)} features: {missing_features}\")\n",
    "    \n",
    "    # Create dataset with BOTH identifiers AND features\n",
    "    all_cols = existing_identifiers + existing_features\n",
    "    df_subset = df[all_cols].copy()\n",
    "\n",
    "    print(f\"\\nüîç Missing value summary:\")\n",
    "    # Check missing values ONLY in feature columns\n",
    "    features_only = df_subset[existing_features]\n",
    "    total_missing = features_only.isna().sum().sum()\n",
    "    total_cells = features_only.shape[0] * features_only.shape[1]\n",
    "    missing_pct = (total_missing / total_cells) * 100\n",
    "    print(f\"  Total missing values in FEATURES: {total_missing:,} ({missing_pct:.2f}% of feature cells)\")\n",
    "\n",
    "    # Drop rows with ANY missing values IN FEATURE COLUMNS ONLY\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"REMOVING ROWS WITH MISSING VALUES (checking features only)\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    before_rows = len(df_subset)\n",
    "    # Get mask of complete rows based on features only\n",
    "    complete_features_mask = features_only.notna().all(axis=1)\n",
    "    df_clean = df_subset[complete_features_mask].copy()\n",
    "    after_rows = len(df_clean)\n",
    "\n",
    "    print(f\"  Before: {before_rows:,} rows\")\n",
    "    print(f\"  After:  {after_rows:,} rows\")\n",
    "    print(f\"  Retained: {after_rows/before_rows*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚úì FINAL DATASET READY FOR CLUSTERING\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Observations: {len(df_clean):,}\")\n",
    "    print(f\"  Identifiers: {len(existing_identifiers)} columns - {existing_identifiers}\")\n",
    "    print(f\"  Features: {len(existing_features)} columns\")\n",
    "    print(f\"  Total columns: {len(df_clean.columns)}\")\n",
    "    print(f\"  Missing values: {df_clean.isna().sum().sum()}\")\n",
    "    print(f\"\\n  Ready for: Z-score standardization ‚Üí UMAP ‚Üí HDBSCAN\")\n",
    "    \n",
    "    # Return COMPLETE dataframe (identifiers + features)\n",
    "    return df_clean, existing_features\n",
    "\n",
    "def assign_sticky_bank_tiers(df, asset_col='total_assets', min_consecutive_quarters=3):\n",
    "    \"\"\"\n",
    "    Assign bank tiers with stickiness - requires crossing threshold for \n",
    "    min_consecutive_quarters before tier changes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame with columns [rssd9017, year, quarter, asset_col]\n",
    "    asset_col : str, name of the assets column\n",
    "    min_consecutive_quarters : int, number of consecutive quarters needed to change tier\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : DataFrame with new 'bank_tier' column\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ASSIGNING STICKY BANK TIERS ({min_consecutive_quarters} consecutive quarters)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Tier thresholds (in thousands)\n",
    "    SMALL_THRESHOLD = 1_000_000\n",
    "    MEDIUM_THRESHOLD = 10_000_000\n",
    "    \n",
    "    def get_raw_tier(assets):\n",
    "        \"\"\"Get tier based on current asset value\"\"\"\n",
    "        if pd.isna(assets):\n",
    "            return None\n",
    "        elif assets < SMALL_THRESHOLD:\n",
    "            return \"Small\"\n",
    "        elif assets < MEDIUM_THRESHOLD:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"Large\"\n",
    "    \n",
    "    # Sort by bank and time\n",
    "    df = df.sort_values(['rssd9017', 'year', 'quarter']).copy()\n",
    "    \n",
    "    # Calculate raw tier for each observation\n",
    "    df['raw_tier'] = df[asset_col].apply(get_raw_tier)\n",
    "    \n",
    "    # Initialize sticky tier column\n",
    "    df['bank_tier'] = None\n",
    "    \n",
    "    # Process each bank separately\n",
    "    tier_changes = 0\n",
    "    total_banks = df['rssd9017'].nunique()\n",
    "    \n",
    "    for bank_id in df['rssd9017'].unique():\n",
    "        bank_mask = df['rssd9017'] == bank_id\n",
    "        bank_data = df.loc[bank_mask].copy()\n",
    "        \n",
    "        # Start with first observation's tier\n",
    "        current_tier = bank_data['raw_tier'].iloc[0]\n",
    "        df.loc[bank_mask, 'bank_tier'] = current_tier\n",
    "        \n",
    "        consecutive_count = 0\n",
    "        potential_new_tier = None\n",
    "        \n",
    "        # Iterate through each quarter for this bank\n",
    "        for idx in bank_data.index[1:]:\n",
    "            raw_tier = df.loc[idx, 'raw_tier']\n",
    "            \n",
    "            # If raw tier suggests a change\n",
    "            if raw_tier != current_tier:\n",
    "                # If this is the same potential change as before, increment counter\n",
    "                if raw_tier == potential_new_tier:\n",
    "                    consecutive_count += 1\n",
    "                else:\n",
    "                    # New potential change, reset counter\n",
    "                    potential_new_tier = raw_tier\n",
    "                    consecutive_count = 1\n",
    "                \n",
    "                # If we've hit the threshold, make the change\n",
    "                if consecutive_count >= min_consecutive_quarters:\n",
    "                    current_tier = potential_new_tier\n",
    "                    tier_changes += 1\n",
    "                    consecutive_count = 0\n",
    "                    potential_new_tier = None\n",
    "            else:\n",
    "                # Raw tier matches current tier, reset any pending change\n",
    "                consecutive_count = 0\n",
    "                potential_new_tier = None\n",
    "            \n",
    "            # Assign current sticky tier\n",
    "            df.loc[idx, 'bank_tier'] = current_tier\n",
    "    \n",
    "    print(f\"‚úì Processed {total_banks:,} banks\")\n",
    "    print(f\"‚úì Total tier changes: {tier_changes:,}\")\n",
    "    print(f\"\\nTier distribution:\")\n",
    "    tier_counts = df.groupby('bank_tier')['rssd9017'].nunique()\n",
    "    for tier in ['Small', 'Medium', 'Large']:\n",
    "        if tier in tier_counts.index:\n",
    "            count = tier_counts[tier]\n",
    "            pct = (count / total_banks) * 100\n",
    "            print(f\"  {tier:8s}: {count:>6,} banks ({pct:>5.1f}%)\")\n",
    "    \n",
    "    # Drop the temporary raw_tier column\n",
    "    df = df.drop(columns=['raw_tier'])\n",
    "    \n",
    "    return df\n",
    "def calculate_additional_innovation_ratios(df):\n",
    "    \"\"\"Calculate additional innovation and efficiency metrics\"\"\"\n",
    "    \n",
    "    ratios = df.copy()\n",
    "    \n",
    "    def safe_divide(numerator, denominator):\n",
    "        result = numerator / denominator\n",
    "        result = result.replace([np.inf, -np.inf], np.nan)\n",
    "        return result\n",
    "    \n",
    "    # === ADDITIONAL INNOVATION METRICS ===\n",
    "    \n",
    "    # Digital revenue intensity (credit card fees relative to total revenue)\n",
    "    ratios['digital_revenue_ratio'] = safe_divide(\n",
    "        df['riad4415'],  # Credit card fees\n",
    "        df['riad4074'] + df['riad4079']  # Total revenue\n",
    "    ) * 100\n",
    "    print(\"‚úì Digital Revenue Ratio\")\n",
    "    \n",
    "    # Non-branch revenue (noninterest income minus service charges)\n",
    "    ratios['non_branch_revenue_pct'] = safe_divide(\n",
    "        df['riad4079'] - df['riad4080'],  # Noninterest income minus service charges\n",
    "        df['riad4074'] + df['riad4079']\n",
    "    ) * 100\n",
    "    print(\"‚úì Non-Branch Revenue %\")\n",
    "    \n",
    "    # Loan efficiency (interest income per dollar of loans)\n",
    "    ratios['loan_yield'] = safe_divide(\n",
    "        df['riad4107'],  # Interest income on loans\n",
    "        df['total_loans']\n",
    "    ) * 100\n",
    "    print(\"‚úì Loan Yield\")\n",
    "    \n",
    "    # Securities intensity (investment in securities relative to assets)\n",
    "    ratios['securities_to_assets'] = safe_divide(\n",
    "        df['htm_securities'] + df['afs_securities'],\n",
    "        df['total_assets']\n",
    "    ) * 100\n",
    "    print(\"‚úì Securities to Assets\")\n",
    "    \n",
    "    # Operating leverage (noninterest expense per salary dollar)\n",
    "    ratios['expense_per_salary_dollar'] = safe_divide(\n",
    "        df['riad4093'],  # Total noninterest expense\n",
    "        df['riad4135']  # Salaries\n",
    "    )\n",
    "    print(\"‚úì Expense per Salary Dollar\")\n",
    "    \n",
    "    # Occupancy efficiency (occupancy expense relative to total assets)\n",
    "    ratios['occupancy_intensity'] = safe_divide(\n",
    "        df['riad4115'],  # Occupancy expense\n",
    "        df['total_assets']\n",
    "    ) * 1000\n",
    "    print(\"‚úì Occupancy Intensity\")\n",
    "    \n",
    "    # Credit quality indicators\n",
    "    ratios['chargeoff_rate'] = safe_divide(\n",
    "        df['riad4635'],  # Total charge-offs\n",
    "        df['total_loans']\n",
    "    ) * 100\n",
    "    print(\"‚úì Charge-off Rate\")\n",
    "    \n",
    "    ratios['provision_intensity'] = safe_divide(\n",
    "        df['riad4230'],  # Provision for loan losses\n",
    "        df['total_loans']\n",
    "    ) * 100\n",
    "    print(\"‚úì Provision Intensity\")\n",
    "    \n",
    "    # Capital efficiency\n",
    "    ratios['asset_growth_capacity'] = safe_divide(\n",
    "        df['total_equity'],\n",
    "        df['total_assets']\n",
    "    ) * 100\n",
    "    print(\"‚úì Asset Growth Capacity\")\n",
    "    \n",
    "    print(f\"\\n‚úì Created 9 additional innovation/efficiency ratios\")\n",
    "    return ratios\n",
    "\n",
    "def calculate_innovation_change_scores(df, feature_list, min_years=10):\n",
    "    \"\"\"\n",
    "    Calculate change in innovation metrics from first to last year for each bank.\n",
    "    Each bank will appear ONCE in the output.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame with bank-year observations\n",
    "    feature_list : list of features to calculate changes for\n",
    "    min_years : minimum number of years a bank must have data for (default 10)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df_changes : DataFrame with one row per bank showing feature changes\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CALCULATING INNOVATION CHANGE SCORES (2010-2021)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    bank_changes = []\n",
    "    banks_processed = 0\n",
    "    banks_excluded = 0\n",
    "    \n",
    "    for bank_id in df['rssd9017'].unique():\n",
    "        bank_data = df[df['rssd9017'] == bank_id].sort_values('year')\n",
    "        \n",
    "        # Require minimum years of data\n",
    "        if len(bank_data) >= min_years:\n",
    "            # Get first and last year data\n",
    "            first_year = bank_data.iloc[0]\n",
    "            last_year = bank_data.iloc[-1]\n",
    "            \n",
    "            changes = {}\n",
    "            changes['rssd9017'] = bank_id\n",
    "            changes['rssd9017_name'] = first_year['rssd9017'] if 'rssd9017' in bank_data.columns else bank_id\n",
    "            changes['bank_tier'] = last_year['bank_tier']\n",
    "            changes['first_year'] = first_year['year']\n",
    "            changes['last_year'] = last_year['year']\n",
    "            changes['years_observed'] = len(bank_data)\n",
    "            \n",
    "            # Calculate change for each feature\n",
    "            for feat in feature_list:\n",
    "                if feat in bank_data.columns:\n",
    "                    changes[f'{feat}_change'] = last_year[feat] - first_year[feat]\n",
    "                    changes[f'{feat}_first'] = first_year[feat]\n",
    "                    changes[f'{feat}_last'] = last_year[feat]\n",
    "            \n",
    "            bank_changes.append(changes)\n",
    "            banks_processed += 1\n",
    "        else:\n",
    "            banks_excluded += 1\n",
    "    \n",
    "    df_changes = pd.DataFrame(bank_changes)\n",
    "    \n",
    "    print(f\"\\n‚úì Processed {banks_processed:,} banks\")\n",
    "    print(f\"‚úó Excluded {banks_excluded:,} banks (less than {min_years} years of data)\")\n",
    "    print(f\"\\nTier distribution:\")\n",
    "    tier_counts = df_changes['bank_tier'].value_counts()\n",
    "    for tier in ['Small', 'Medium', 'Large']:\n",
    "        if tier in tier_counts.index:\n",
    "            print(f\"  {tier:8s}: {tier_counts[tier]:>6,} banks\")\n",
    "    \n",
    "    # Check for missing values in change scores\n",
    "    change_cols = [col for col in df_changes.columns if col.endswith('_change')]\n",
    "    missing_pct = (df_changes[change_cols].isna().sum().sum() / \n",
    "                   (len(df_changes) * len(change_cols))) * 100\n",
    "    print(f\"\\nMissing values in change scores: {missing_pct:.2f}%\")\n",
    "    \n",
    "    return df_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "662806ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MERGING SPLIT TIME CODES (2011 Transition)\n",
      "================================================================================\n",
      "‚úì total_assets              | RCFD:  4,201 | RCON: 292,714 | Total: 296,463\n",
      "‚úì total_loans               | RCFD:  4,201 | RCON: 296,463 | Total: 296,463\n",
      "‚úì total_equity              | RCFD:  4,201 | RCON: 292,262 | Total: 296,463\n",
      "‚úì allowance_loan_losses     | RCFD:  4,201 | RCON: 292,262 | Total: 296,463\n",
      "‚úì agricultural_loans        | RCFD:  4,201 | RCON: 296,463 | Total: 296,463\n",
      "‚úì htm_securities            | RCFD:  4,201 | RCON: 296,217 | Total: 296,463\n",
      "‚úì afs_securities            | RCFD:  4,201 | RCON: 296,217 | Total: 296,463\n",
      "‚úì goodwill                  | RCFD:  4,201 | RCON: 292,262 | Total: 296,463\n",
      "‚úì cash_items_process        | RCFD:  4,201 | RCON: 292,262 | Total: 296,463\n",
      "‚úì farmland_loans            | RCFD:  1,720 | RCON: 296,463 | Total: 296,463\n",
      "‚úì multifamily_loans         | RCFD:  1,720 | RCON: 296,463 | Total: 296,463\n",
      "\n",
      "‚úì Successfully merged/renamed 11 split column pairs\n",
      "\n",
      "================================================================================\n",
      "CALCULATING FINANCIAL RATIOS (USING MERGED COLUMNS)\n",
      "================================================================================\n",
      "‚úì Tech Investment Ratio\n",
      "‚úì NIB Deposit Ratio (Digital Banking Proxy)\n",
      "‚úì Service Charge Intensity\n",
      "‚úì Efficiency Ratio\n",
      "‚úì Noninterest Income %\n",
      "‚úì Loans-to-Assets\n",
      "‚úì Equity-to-Assets\n",
      "‚úì Deposits-to-Assets\n",
      "‚úì ROA\n",
      "‚úì ROE\n",
      "‚úì Nontransaction Deposits %\n",
      "\n",
      "‚úì Created 11 ratios using merged columns\n",
      "‚úì Digital Revenue Ratio\n",
      "‚úì Non-Branch Revenue %\n",
      "‚úì Loan Yield\n",
      "‚úì Securities to Assets\n",
      "‚úì Expense per Salary Dollar\n",
      "‚úì Occupancy Intensity\n",
      "‚úì Charge-off Rate\n",
      "‚úì Provision Intensity\n",
      "‚úì Asset Growth Capacity\n",
      "\n",
      "‚úì Created 9 additional innovation/efficiency ratios\n",
      "\n",
      "================================================================================\n",
      "PREPARING CLUSTERING FEATURES - UNIVERSAL COVERAGE ONLY\n",
      "================================================================================\n",
      "\n",
      "üìä Feature selection:\n",
      "  Expected: 68 features\n",
      "  Found: 68 features\n",
      "\n",
      "üîç Missing value summary:\n",
      "  Total missing values in FEATURES: 27,337 (0.14% of feature cells)\n",
      "\n",
      "================================================================================\n",
      "REMOVING ROWS WITH MISSING VALUES (checking features only)\n",
      "================================================================================\n",
      "  Before: 296,463 rows\n",
      "  After:  289,946 rows\n",
      "  Retained: 97.8%\n",
      "\n",
      "================================================================================\n",
      "‚úì FINAL DATASET READY FOR CLUSTERING\n",
      "================================================================================\n",
      "  Observations: 289,946\n",
      "  Identifiers: 5 columns - ['rssd9001', 'rssd9999', 'rssd9017', 'year', 'quarter']\n",
      "  Features: 68 columns\n",
      "  Total columns: 73\n",
      "  Missing values: 0\n",
      "\n",
      "  Ready for: Z-score standardization ‚Üí UMAP ‚Üí HDBSCAN\n",
      "\n",
      "================================================================================\n",
      "ASSIGNING STICKY BANK TIERS (3 consecutive quarters)\n",
      "================================================================================\n",
      "‚úì Processed 7,751 banks\n",
      "‚úì Total tier changes: 744\n",
      "\n",
      "Tier distribution:\n",
      "  Small   :  7,020 banks ( 90.6%)\n",
      "  Medium  :  1,184 banks ( 15.3%)\n",
      "  Large   :    207 banks (  2.7%)\n",
      "\n",
      "‚úì Bank-year aggregated: 64,645 observations\n",
      "\n",
      "‚úì Using 11 innovation-only features for change scores\n",
      "\n",
      "================================================================================\n",
      "CALCULATING INNOVATION CHANGE SCORES (2010-2021)\n",
      "================================================================================\n",
      "\n",
      "‚úì Processed 4,348 banks\n",
      "‚úó Excluded 3,403 banks (less than 9 years of data)\n",
      "\n",
      "Tier distribution:\n",
      "  Small   :  3,658 banks\n",
      "  Medium  :    619 banks\n",
      "  Large   :     71 banks\n",
      "\n",
      "Missing values in change scores: 0.00%\n",
      "\n",
      "‚úì Ready to cluster on 11 change features\n",
      "‚úì Dataset: 4,348 banks (each appears once)\n"
     ]
    }
   ],
   "source": [
    "# === WORKFLOW WITH NEW RATIOS AND CHANGE SCORES ===\n",
    "\n",
    "# Load and prepare data\n",
    "df = data.copy()\n",
    "df['report_date'] = pd.to_datetime(data['rssd9999'], errors='coerce')\n",
    "df['year'] = df['report_date'].dt.year\n",
    "df['quarter'] = df['report_date'].dt.quarter\n",
    "\n",
    "# Step 1: Merge split columns\n",
    "df = merge_split_columns(df)\n",
    "\n",
    "# Step 2: Calculate original ratios\n",
    "df_ratios = calculate_ratios(df)\n",
    "\n",
    "# Step 3: Calculate additional innovation ratios\n",
    "df_ratios = calculate_additional_innovation_ratios(df_ratios)\n",
    "\n",
    "# Step 4: Prepare features for clustering\n",
    "df_umap, feature_names = prepare_clustering_features(df_ratios)\n",
    "\n",
    "# Step 5: Assign sticky bank tiers\n",
    "df_umap = assign_sticky_bank_tiers(df_umap, asset_col='total_assets', min_consecutive_quarters=3)\n",
    "\n",
    "# Step 6: Aggregate to bank-year level\n",
    "bank_year_aggregated = df_umap.groupby(['rssd9017', 'year', 'bank_tier'])[feature_names].mean().reset_index()\n",
    "\n",
    "print(f\"\\n‚úì Bank-year aggregated: {len(bank_year_aggregated):,} observations\")\n",
    "\n",
    "# Step 7: Define innovation-only features (size-independent ratios)\n",
    "innovation_only_features = [\n",
    "    # Original ratios\n",
    "    'tech_investment_ratio',\n",
    "    'nib_deposit_ratio', \n",
    "    'service_charge_intensity',\n",
    "    'efficiency_ratio',\n",
    "    'nonint_income_pct',\n",
    "    'loans_to_assets',\n",
    "    'equity_to_assets',\n",
    "    'deposits_to_assets',\n",
    "    'roa',\n",
    "    'roe',\n",
    "    'nontrans_deposits_pct',\n",
    "    \n",
    "    # New ratios\n",
    "    'digital_revenue_ratio',\n",
    "    'non_branch_revenue_pct',\n",
    "    'loan_yield',\n",
    "    'securities_to_assets',\n",
    "    'expense_per_salary_dollar',\n",
    "    'occupancy_intensity',\n",
    "    'chargeoff_rate',\n",
    "    'provision_intensity',\n",
    "    'asset_growth_capacity'\n",
    "]\n",
    "\n",
    "# Filter to only features that exist\n",
    "innovation_features_available = [f for f in innovation_only_features if f in bank_year_aggregated.columns]\n",
    "print(f\"\\n‚úì Using {len(innovation_features_available)} innovation-only features for change scores\")\n",
    "\n",
    "# Step 8: Calculate change scores (each bank appears ONCE)\n",
    "df_changes = calculate_innovation_change_scores(\n",
    "    bank_year_aggregated, \n",
    "    innovation_features_available,\n",
    "    min_years=9\n",
    ")\n",
    "\n",
    "# Step 9: Prepare change scores for clustering\n",
    "change_feature_cols = [col for col in df_changes.columns if col.endswith('_change')]\n",
    "\n",
    "print(f\"\\n‚úì Ready to cluster on {len(change_feature_cols)} change features\")\n",
    "print(f\"‚úì Dataset: {len(df_changes):,} banks (each appears once)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "348378f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Large banks: 71 observations\n",
      "  Clusters found: 2\n",
      "  Noise points: 16 (22.5%)\n",
      "\n",
      "Processing Medium banks: 619 observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jdorv\\Coding Fun\\Bank Innovation\\Final-project\\.venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Clusters found: 5\n",
      "  Noise points: 236 (38.1%)\n",
      "\n",
      "Processing Small banks: 3,658 observations\n",
      "  Clusters found: 3\n",
      "  Noise points: 745 (20.4%)\n",
      "\n",
      "‚úì Clustering complete!\n"
     ]
    }
   ],
   "source": [
    "# Cluster by tier using CHANGE SCORES\n",
    "by_tier = df_changes.groupby(\"bank_tier\")\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean', random_state=42)\n",
    "embedding = {}\n",
    "clusters = {}\n",
    "\n",
    "for tier, data in by_tier:\n",
    "    print(f\"\\nProcessing {tier} banks: {len(data):,} observations\")\n",
    "    \n",
    "    # Use only change score features\n",
    "    scaled = StandardScaler().fit_transform(data[change_feature_cols])\n",
    "    embedding[tier] = reducer.fit_transform(scaled)\n",
    "    \n",
    "    # Adjust HDBSCAN params based on tier size\n",
    "    if tier == 'Large':\n",
    "        min_cluster_size = 15  # Lower for small sample\n",
    "        min_samples = 3\n",
    "    elif tier == 'Medium':\n",
    "        min_cluster_size = 30\n",
    "        min_samples = 5\n",
    "    else:  # Small\n",
    "        min_cluster_size = 50\n",
    "        min_samples = 10\n",
    "    \n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size, \n",
    "        min_samples=min_samples,\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    clusters[tier] = clusterer.fit_predict(embedding[tier])\n",
    "    \n",
    "    n_clusters = len(set(clusters[tier])) - (1 if -1 in clusters[tier] else 0)\n",
    "    n_noise = (clusters[tier] == -1).sum()\n",
    "    \n",
    "    print(f\"  Clusters found: {n_clusters}\")\n",
    "    print(f\"  Noise points: {n_noise:,} ({n_noise/len(data)*100:.1f}%)\")\n",
    "\n",
    "# Add results back to dataframe\n",
    "df_changes['innovation_cluster'] = -1\n",
    "df_changes['umap_1'] = np.nan\n",
    "df_changes['umap_2'] = np.nan\n",
    "\n",
    "for tier, data in by_tier:\n",
    "    df_changes.loc[data.index, 'innovation_cluster'] = clusters[tier]\n",
    "    df_changes.loc[data.index, 'umap_1'] = embedding[tier][:, 0]\n",
    "    df_changes.loc[data.index, 'umap_2'] = embedding[tier][:, 1]\n",
    "\n",
    "print(f\"\\n‚úì Clustering complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99352a2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 1 (1687162075.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after function definition on line 1\n"
     ]
    }
   ],
   "source": [
    "def analyze_cluster_by_size(df, tier):\n",
    "#\"Check if clusters within a tier are just grouping by asset size\"\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"CLUSTER vs ASSET SIZE ANALYSIS - {tier} Banks\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "tier_data = df[df['bank_tier'] == tier].copy()\n",
    "\n",
    "# Get clusters (excluding noise)\n",
    "clusters = [c for c in tier_data['innovation_cluster'].unique() if c != -1]\n",
    "clusters.sort()\n",
    "\n",
    "print(f\"\\nAsset statistics by cluster:\")\n",
    "print(f\"{'Cluster':<10} {'Count':>8} {'Mean Assets':>15} {'Median Assets':>15} {'Min Assets':>15} {'Max Assets':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_data = tier_data[tier_data['innovation_cluster'] == cluster]\n",
    "    assets = cluster_data['total_assets']\n",
    "    \n",
    "    print(f\"{cluster:<10} {len(cluster_data):>8,} {assets.mean():>15,.0f} {assets.median():>15,.0f} {assets.min():>15,.0f} {assets.max():>15,.0f}\")\n",
    "\n",
    "# Handle noise separately if it exists\n",
    "if -1 in tier_data['innovation_cluster'].values:\n",
    "    noise_data = tier_data[tier_data['innovation_cluster'] == -1]\n",
    "    assets = noise_data['total_assets']\n",
    "    print(f\"{'Noise':<10} {len(noise_data):>8,} {assets.mean():>15,.0f} {assets.median():>15,.0f} {assets.min():>15,.0f} {assets.max():>15,.0f}\")\n",
    "\n",
    "# Statistical test: ANOVA on assets across clusters\n",
    "from scipy import stats\n",
    "cluster_assets = [tier_data[tier_data['innovation_cluster'] == c]['total_assets'].values \n",
    "                    for c in clusters]\n",
    "\n",
    "if len(clusters) > 1:\n",
    "    f_stat, p_value = stats.f_oneway(*cluster_assets)\n",
    "    print(f\"\\nANOVA test for asset size across clusters:\")\n",
    "    print(f\"  F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.6f}\")\n",
    "    \n",
    "    if p_value < 0.001:\n",
    "        print(f\"  ‚ö†Ô∏è  WARNING: Clusters have significantly different asset sizes (p < 0.001)\")\n",
    "        print(f\"     This suggests clustering may be driven by size, not innovation\")\n",
    "    else:\n",
    "        print(f\"  ‚úì Clusters have similar asset distributions (p >= 0.001)\")\n",
    "\n",
    "# Run for each tier\n",
    "for tier in ['Large', 'Medium', 'Small']:\n",
    "    analyze_cluster_by_size(bank_year_aggregated, tier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
